# Machine Learning in Finance - Group Project

## Project Overview

This project focuses on using machine learning to assess the risk of default for Home Credit loan applications. The goal is to build predictive models that can help identify customers likely to default, enabling better decision-making and optimized loan offerings.

The project is divided into three main tasks:
1. **Task 1**: Data Cleaning and Exploration
2. **Task 2**: Classification Model to Predict Payment Difficulties
3. **Task 3**: Customer Segmentation

## Folder Contents

### Jupyter Notebooks
- **`Project-Task1.ipynb`**: Contains data cleaning, exploration, and preprocessing. This notebook processes the raw data and generates cleaned datasets for subsequent tasks.
- **`Project-Task2.ipynb`**: Implements classification models to predict loan defaults. Includes multiple algorithms (Logistic Regression, Decision Trees, Random Forest, Gradient Boosting, LightGBM) with hyperparameter tuning using Optuna.
- **`Project-Task3.ipynb`**: Applies unsupervised learning techniques (K-Means, Agglomerative Clustering, DBSCAN) for customer segmentation.

### Data Files
- **`project_data.csv`**: Raw dataset containing the Home Credit Default Risk data (input for Task 1).
- **`project_desc.csv`**: Data dictionary providing descriptions of column titles in the dataset.
- **`task2_classification_data.csv`**: Cleaned and preprocessed data for classification modeling (generated by Task 1, used as input for Task 2).
- **`task3_segmentation_data.csv`**: Cleaned and preprocessed data for customer segmentation (generated by Task 1, used as input for Task 3).

### Documentation
- **`Project 25-26.pdf`**: Project instructions and requirements document.

### Configuration Files
- **`requirements.txt`**: Python package dependencies required to run the notebooks.
- **`venv/`**: Virtual environment directory containing the project's Python environment.

## Data Paths Configuration

To run the notebooks, you need to update the data file paths in the following locations:

### Task 1 (Project-Task1.ipynb)
**Location**: Cell 3 (Data Loading section)
```python
data = pd.read_csv('/home/renato/Documents/Mestrado/ML4Finance/Projeto Grupo/Group Project-20251220/project_data.csv')
```
**Update to**: Your absolute path to `project_data.csv`

### Task 2 (Project-Task2.ipynb)
**Location**: Cell 5 (Data Loading and Preparation section)
```python
data_path = '/home/renato/Documents/Mestrado/ML4Finance/Projeto Grupo/Group Project-20251220/task2_classification_data.csv'
data = pd.read_csv(data_path)
```
**Update to**: Your absolute path to `task2_classification_data.csv`

### Task 3 (Project-Task3.ipynb)
**Location**: Cell 5 (Data Loading and Preparation section)
```python
data_path = '/home/renato/Documents/Mestrado/ML4Finance/Projeto Grupo/Group Project-20251220/task3_segmentation_data.csv'
data = pd.read_csv(data_path)
```
**Update to**: Your absolute path to `task3_segmentation_data.csv`

**Note**: Make sure to run Task 1 first, as it generates the cleaned datasets (`task2_classification_data.csv` and `task3_segmentation_data.csv`) that are required for Tasks 2 and 3.

## LightGBM GPU Configuration

The LightGBM models in **Task 2** are configured to use GPU acceleration for faster training. This is specified using the `device_type='gpu'` parameter in the LightGBM classifier initialization.

### Current Configuration (GPU)
The notebooks currently use:
```python
LGBMClassifier(
    device_type='gpu',  # GPU acceleration enabled
    # ... other parameters
)
```

### Switching to CPU
If you want to use CPU instead of GPU (e.g., if you don't have GPU support or want to avoid GPU dependencies), simply **omit the `device_type` parameter** from the LightGBM classifier initialization:

```python
LGBMClassifier(
    # device_type='gpu',  # Remove or comment out this line
    # ... other parameters
)
```

When `device_type` is not specified, LightGBM will default to using CPU.

**Important Notes**:
- GPU support requires CUDA-compatible GPU and appropriate LightGBM GPU build
- CPU mode will work on any system but may be slower for large datasets
- The parameter appears in multiple locations in Task 2 (Optuna tuning and final model building), so make sure to update all instances if switching to CPU

## Setup Instructions

1. **Create and activate a virtual environment** (if not already done):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Linux/Mac
   # or
   venv\Scripts\activate  # On Windows
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Update data paths** in the notebooks as described in the "Data Paths Configuration" section above.

4. **Run the notebooks in order**:
   - Start with `Project-Task1.ipynb` to clean and prepare the data
   - Then run `Project-Task2.ipynb` for classification modeling
   - Finally run `Project-Task3.ipynb` for customer segmentation

## Requirements

The project requires Python 3.12+ and the following packages (see `requirements.txt` for versions):
- pandas
- numpy
- matplotlib
- seaborn
- scikit-learn
- lightgbm
- optuna
- scipy
- jupyter

## Group Members

1. 20221960 - Renato Bernardino
2. 20221899 - David Duarte

